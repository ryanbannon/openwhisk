# -*- coding: utf-8 -*-
"""Experiment 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lDwvESzRt5dCJw5gOw1zu-XgDdNwIfYX

# **Imports**
"""

from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import csv
import numpy as np
from datetime import datetime, timedelta

drive.mount('/content/gdrive')

"""# **Data Prep**"""

data = pd.read_csv('gdrive/My Drive/Colab Notebooks/data/AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv')
data['counter'] = range(1,len(data)+1)
data['start_timestamp'] = (data['end_timestamp']-data['duration'])
date = '2021-01-31'
time = '00:00:00.0'
start_date = datetime.strptime(date + ' ' + time, "%Y-%m-%d %H:%M:%S.%f")
data['datetime'] = start_date + pd.TimedeltaIndex(data['start_timestamp'], unit='s')
data['date'] = data['datetime']
data = data.set_index('datetime')
data['DatetimeNumeric'] = pd.to_datetime(data['date'])
data['DatetimeNumeric'] = data['DatetimeNumeric'].map(datetime.timestamp)
data['wait'] = round(data['start_timestamp'].diff()*1000,0)
data.wait = data.wait.fillna(0).astype(int)
data

def interQuartile(df_):
  for x in ['wait']:
      q75,q25 = np.percentile(df_.loc[:,x],[75,25])
      intr_qr = q75-q25
  
      max = q75+(1.5*intr_qr)
      min = q25-(1.5*intr_qr)
  
      df_.loc[df_[x] < min,x] = np.nan
      df_.loc[df_[x] > max,x] = np.nan

  df_ = df_.dropna(axis = 0)
  df_.isnull().sum()
  return df_

# 04/02/2021 12:00 - 18:00

df_1 = data.copy()
start_date_obj = datetime.strptime('2021-02-04 12', "%Y-%m-%d %H")
end_date_obj = datetime.strptime('2021-02-04 18', "%Y-%m-%d %H")
mask = (df_1['date'] > start_date_obj) & (df_1['date'] <= end_date_obj)
df_1 = df_1.loc[mask]

# 31/01/2021 00:00 - 12/02/2021 18:00

df1 = data.copy()
start_date_obj = datetime.strptime('2021-01-31', "%Y-%m-%d")
end_date_obj = datetime.strptime('2021-02-04 18', "%Y-%m-%d %H")
# setting test count
end_temp = end_date_obj - timedelta(hours=6)
test = (df1['date'] > end_temp) & (df1['date'] <= end_date_obj)
test_count = len(df1.loc[test])
# continuing with data frame prep
mask = (df1['date'] > start_date_obj) & (df1['date'] <= end_date_obj)
df1 = df1.loc[mask]

df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
df_1 = interQuartile(df_1)
wait_col = ['wait']
df_1.boxplot(wait_col)

df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
df1 = interQuartile(df1)
wait_col = ['wait']
df1.boxplot(wait_col)

"""### **Azure Dataset for JMeter**"""

df_1_copy = df_1.copy()
df_1_copy = df_1_copy.drop(['counter', 'DatetimeNumeric', 'wait'], axis=1)
df_1_copy = df_1_copy[::4]
df_1_copy = df_1_copy[::2]
df_1_copy['func'] = '155e47f8e7f751d0c845049456d01832013c61336a8cd85901330ac821a71534'
df_1_copy['counter'] = range(1,len(df_1_copy)+1)
df_1_copy['wait'] = round(df_1_copy['start_timestamp'].diff()*1000,0)
df_1_copy.wait = df_1_copy.wait.fillna(0).astype(int)
df_1_copy

df_1_copy.to_csv('gdrive/My Drive/Colab Notebooks/data/Experiment_1_2_AzureDataset.csv', sep=',', encoding='utf-8')

"""# **Linear Regression**"""

events = df1['counter']
time = df1['DatetimeNumeric']
plt.scatter(events, time)
plt.xlabel('events')
plt.ylabel('time')

def get_predictions(model, x):
  alpha_hat = model['alpha_hat']
  beta_hat = model['beta_hat']
  return alpha_hat + beta_hat * x

def get_predictions_list(n):
  n = n + 1
  lst = []
  for i in range(max(events)+1,max(events)+n):
      lst.append(i)
  return lst

def mean_squared_error(y, y_predictions):
  n = len(y)
  return np.sum(np.square(y - y_predictions)) / n

def mean_absolute_error(y, y_predictions):
  n = len(y)
  return np.sum(np.abs(y - y_predictions)) / n

def get_best_model(x, y):
  x_bar = np.average(x)
  y_bar = np.average(y)
  top = np.sum((x - x_bar)*(y - y_bar))
  bot = np.sum((x - x_bar)**2)
  beta_hat = top / bot
  alpha_hat = y_bar - beta_hat*x_bar
  model = {'alpha_hat':alpha_hat, 'beta_hat':beta_hat}
  return model

events_np = events.to_numpy()
time_np = time.to_numpy()
best_model = get_best_model(events_np, time_np)
print(best_model)
mse1 = mean_squared_error(get_predictions(best_model, events_np), events)
print(mse1)
mae1 = mean_absolute_error(get_predictions(best_model, events_np), events)
print(mae1)

n_predictions = test_count
s = [1**n for n in range(len(events))]
s_predictions = [1**n for n in range(n_predictions)]
events = df1['counter']
time = df1['DatetimeNumeric']

plt.scatter(events, time, s=s, label='train values')
plt.scatter(events, get_predictions(best_model, events_np), s=s, label='train predictions')
plt.scatter(np.asarray(get_predictions_list(n=n_predictions)), get_predictions(best_model, np.asarray(get_predictions_list(n=n_predictions))), s=s_predictions, label='future predictions')
plt.xlabel('Executions')
plt.ylabel('Timestamps')
plt.legend(loc="upper left")

"""### **Linear Regression Predictions**"""

predictions = get_predictions(best_model, np.asarray(get_predictions_list(n=n_predictions)))
linearRegression = pd.DataFrame(predictions, columns = ['Predictions'])
linearRegression['wait'] = round(linearRegression['Predictions'].diff()*1000,0)
linearRegression['wait'] = linearRegression['wait'].fillna(linearRegression['wait'][1])
linearRegression.tail()

linearRegression_copy = (linearRegression['wait']*20).astype(int)
linearRegression_copy = linearRegression_copy[::4]
linearRegression_copy = linearRegression_copy[::2]
linearRegression_copy.to_csv('gdrive/My Drive/Colab Notebooks/data/experiment_1_2_times.csv', sep=',', encoding='utf-8')