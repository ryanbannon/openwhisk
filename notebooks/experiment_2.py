# -*- coding: utf-8 -*-
"""Experiment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HbBdUG5QfbV3CGY9QGSV42KF6xMHB9w7

# **Imports**
"""

from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import csv
import numpy as np
from datetime import datetime, timedelta
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model

drive.mount('/content/gdrive')

"""# **Data Prep**"""

data = pd.read_csv('gdrive/My Drive/Colab Notebooks/data/AzureFunctionsInvocationTraceForTwoWeeksJan2021.csv')
data['counter'] = range(1,len(data)+1)
data['start_timestamp'] = (data['end_timestamp']-data['duration'])
date = '2021-01-31'
time = '00:00:00.0'
start_date = datetime.strptime(date + ' ' + time, "%Y-%m-%d %H:%M:%S.%f")
data['datetime'] = start_date + pd.TimedeltaIndex(data['start_timestamp'], unit='s')
data['date'] = data['datetime']
data = data.set_index('datetime')
data['DatetimeNumeric'] = pd.to_datetime(data['date'])
data['DatetimeNumeric'] = data['DatetimeNumeric'].map(datetime.timestamp)
data['wait'] = round(data['start_timestamp'].diff()*1000,0)
data.wait = data.wait.fillna(0).astype(int)
data

def interQuartile(df_):
  for x in ['wait']:
      q75,q25 = np.percentile(df_.loc[:,x],[75,25])
      intr_qr = q75-q25
  
      max = q75+(1.5*intr_qr)
      min = q25-(1.5*intr_qr)
  
      df_.loc[df_[x] < min,x] = np.nan
      df_.loc[df_[x] > max,x] = np.nan

  df_ = df_.dropna(axis = 0)
  df_.isnull().sum()
  return df_

# 12/02/2021 12:00 - 18:00

df_2 = data.copy()
start_date_obj = datetime.strptime('2021-02-12 12', "%Y-%m-%d %H")
end_date_obj = datetime.strptime('2021-02-12 18', "%Y-%m-%d %H")
mask = (df_2['date'] > start_date_obj) & (df_2['date'] <= end_date_obj)
df_2 = df_2.loc[mask]

# 31/01/2021 00:00 - 12/02/2021 18:00

df2 = data.copy()
start_date_obj = datetime.strptime('2021-01-31', "%Y-%m-%d")
end_date_obj = datetime.strptime('2021-02-12 18', "%Y-%m-%d %H")
mask = (df2['date'] > start_date_obj) & (df2['date'] <= end_date_obj)
df2 = df2.loc[mask]

df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
df_2 = interQuartile(df_2)
wait_col = ['wait']
df_2.boxplot(wait_col)

df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
df2 = interQuartile(df2)
wait_col = ['wait']
df2.boxplot(wait_col)

"""### **Azure Dataset for JMeter**"""

df_2_copy = df_2.copy()
df_2_copy = df_2_copy.drop(['counter', 'DatetimeNumeric', 'wait'], axis=1)
df_2_copy = df_2_copy[::5]
df_2_copy = df_2_copy[::2]
df_2_copy['func'] = '155e47f8e7f751d0c845049456d01832013c61336a8cd85901330ac821a71534'
df_2_copy['counter'] = range(1,len(df_2_copy)+1)
df_2_copy['wait'] = round(df_2_copy['start_timestamp'].diff()*1000,0)
df_2_copy.wait = df_2_copy.wait.fillna(0).astype(int)
df_2_copy.tail()

df_2_copy.to_csv('gdrive/My Drive/Colab Notebooks/data/Experiment_2_2_AzureDataset.csv', sep=',', encoding='utf-8')

"""# **Recurrent Neural Networks**"""

df2 = df2[['wait','DatetimeNumeric']]
df2['hour'] = df2.index.hour
df2['day'] = df2.index.day

# Day
df2['day_sin'] = np.sin(df2.day*(2.*np.pi/30))
df2['day_cos'] = np.cos(df2.day*(2.*np.pi/30))
# Hour
df2['hour_sin'] = np.sin(df2.day*(2.*np.pi/24))
df2['hour_cos'] = np.cos(df2.day*(2.*np.pi/24))
# Minute
df2['minute_sin'] = np.sin(df2.day*(2.*np.pi/60))
df2['minute_cos'] = np.cos(df2.day*(2.*np.pi/60))
df2 = df2.drop(['hour','day','DatetimeNumeric'], axis=1)
df2.tail()

def df_to_X_y(df, window_size=5):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [a for a in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size][0]
    y.append(label)
  return np.array(X), np.array(y)

WINDOW_SIZE = 20
X2, y2 = df_to_X_y(df2, WINDOW_SIZE)
X2.shape, y2.shape

X_train2, y_train2 = X2[:1200000], y2[:1200000]
X_val2, y_val2 = X2[1200000:1244010], y2[1200000:1244010]
X_test2, y_test2 = X2[1244010:], y2[1244010:]
X_train2.shape, y_train2.shape, X_val2.shape, y_val2.shape, X_test2.shape, y_test2.shape

temp_training_mean = np.mean(X_train2[:, :, 0])
temp_training_std = np.std(X_train2[:, :, 0])

def preprocessing(X):
  X[:, :, 0] = (X[:, :, 0] - temp_training_mean) / temp_training_std
  return X

preprocessing(X_train2)
preprocessing(X_val2)
preprocessing(X_test2)

"""## **Long Short Term Memory**"""

model1 = Sequential()
model1.add(InputLayer((X_train2.shape[1],X_train2.shape[2])))
model1.add(LSTM(units=64, activation='relu', stateful=False))
model1.add(Dense(8, 'relu'))
model1.add(Dense(1, 'linear'))
model1.summary()

cp1 = ModelCheckpoint('model1/', save_best_only=True)
model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])

model1.fit(X_train2, y_train2, validation_data=(X_val2, y_val2), epochs=5, callbacks=[cp1])

model1 = load_model('model1/')

"""### **LSTM Train Predictions**"""

train_predictions1 = model1.predict(X_train2).flatten()
train_results1 = pd.DataFrame(data={'Train Predictions':abs(train_predictions1), 'Actuals':y_train2})
plt.plot(train_results1['Train Predictions'][:200],label="Predictions")
plt.plot(train_results1['Actuals'][:200],label="Actuals")
plt.legend(loc="upper right")

"""### **LSTM Test Predictions**"""

test_predictions1 = model1.predict(X_test2).flatten()
test_results1 = pd.DataFrame(data={'Test Predictions':abs(test_predictions1), 'Actuals':y_test2})
plt.plot(test_results1['Test Predictions'][:200],label="Predictions")
plt.plot(test_results1['Actuals'][:200],label="Actuals")
plt.legend(loc="upper right")

"""## **Gated Recurrent Unit**"""

model2 = Sequential()
model2.add(InputLayer((X_train2.shape[1],X_train2.shape[2])))
model2.add(GRU(units=64, activation='relu', stateful=False))
model2.add(Dense(8, 'relu'))
model2.add(Dense(1, 'linear'))
model2.summary()

cp2 = ModelCheckpoint('model2/', save_best_only=True)
model2.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])

model2.fit(X_train2, y_train2, validation_data=(X_val2, y_val2), epochs=5, callbacks=[cp2])

model2 = load_model('model2/')

"""### **GRU Train Predictions**"""

train_predictions2 = model2.predict(X_train2).flatten()
train_results2 = pd.DataFrame(data={'Train Predictions':abs(train_predictions2), 'Actuals':y_train2})
plt.plot(train_results2['Train Predictions'][:200],label="Predictions")
plt.plot(train_results2['Actuals'][:200],label="Actuals")
plt.legend(loc="upper right")

"""### **GRU Test Predictions**"""

test_predictions2 = model2.predict(X_test2).flatten()
test_results2 = pd.DataFrame(data={'wait':abs(test_predictions2), 'Actuals':y_test2})
plt.plot(test_results2['wait'][:200],label="Predictions")
plt.plot(test_results2['Actuals'][:200],label="Actuals")
plt.legend(loc="upper right")

"""### **Prediction Dataset for JMeter**"""

test_results2_copy = (test_results2['wait']*60).astype(int)
test_results2_copy = test_results2_copy[::5]
test_results2_copy = test_results2_copy[::2]
test_results2_copy.to_csv('gdrive/My Drive/Colab Notebooks/data/experiment_2_2_times.csv', sep=',', encoding='utf-8')